{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leedhn/papago/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbyu-PqI4XK2"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amR3er8N48YQ",
        "outputId": "feb7aba1-bac4-4498-934a-808330db63c6"
      },
      "source": [
        "from google.colab import drive #edit\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/NAVER "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/NAVER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DL2QXd4-d8"
      },
      "source": [
        "import pandas as pd #edit\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rh9gdZnkvpp"
      },
      "source": [
        "NUM_EPOCHS = 100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFhLCs6aidcm"
      },
      "source": [
        "#Logger setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzLlnUEce4nL",
        "outputId": "35f02430-10a2-4dec-a598-1902ca30f94a"
      },
      "source": [
        "logger = logging.getLogger('my_logger')\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
        "logger.info('This message has a date/time timestamp')\n",
        "\n",
        "\n",
        "logger.propagate = False # do not pass logs to the default logger\n",
        "\n",
        "# Create handlers\n",
        "c_handler = logging.StreamHandler()\n",
        "EPOCH=10\n",
        "f_handler = logging.FileHandler(f'transformer_{NUM_EPOCHS}.log', mode='w')\n",
        "c_handler.setLevel(logging.INFO)\n",
        "f_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create formatters and add it to handlers\n",
        "c_format = logging.Formatter('%(asctime)s - %(message)s')\n",
        "f_format = logging.Formatter('%(asctime)s - %(message)s')\n",
        "c_handler.setFormatter(c_format)\n",
        "f_handler.setFormatter(f_format)\n",
        "\n",
        "# Add handlers to the logger\n",
        "logger.addHandler(c_handler)\n",
        "logger.addHandler(f_handler)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:35,885 - This message has a date/time timestamp\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eLVMR59igal"
      },
      "source": [
        "#Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm7zkgxU5B4S",
        "outputId": "6f3c2272-6b86-4298-b102-ab174d394ebf"
      },
      "source": [
        "logger.info('='*50)\n",
        "logger.info('LOADING DATA')\n",
        "logger.info('')\n",
        "%cd data\n",
        "txt_list = glob('*.txt') #edit\n",
        "datas = {}\n",
        "for txt in txt_list:\n",
        "    datas[txt] = pd.read_csv(txt,header=None)\n",
        "    name = txt[-10:-4]\n",
        "    datas[txt].columns = [name]\n",
        "    for i in range(len(datas[txt][name])):\n",
        "        datas[txt][name][i] = np.fromstring(datas[txt][name][i] ,dtype=int,sep=' ').tolist()\n",
        "        #datas[txt][name][i].append(1)\n",
        "    logger.info(f'Load {txt} finished')\n",
        "%cd ../\n",
        "\n",
        "logger.info('='*50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:35,904 - ==================================================\n",
            "2021-06-10 13:30:35,907 - LOADING DATA\n",
            "2021-06-10 13:30:35,909 - \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NAVER/data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:36,974 - Load train_source.txt finished\n",
            "2021-06-10 13:30:38,183 - Load train_target.txt finished\n",
            "2021-06-10 13:30:38,419 - Load test_target.txt finished\n",
            "2021-06-10 13:30:38,644 - Load test_source.txt finished\n",
            "2021-06-10 13:30:38,655 - ==================================================\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NAVER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD-AQmMv4XK5"
      },
      "source": [
        "\n",
        "Language Translation with Transformer\n",
        "=====================================\n",
        "\n",
        "This tutorial shows, how to train a translation model from scratch using\n",
        "Transformer. We will be using Multi30k dataset to train a German to English translation model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR1qXp374XK5"
      },
      "source": [
        "Data Processing\n",
        "---------------\n",
        "\n",
        "torchtext has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. In this example, we show how to tokenize a raw text sentence,\n",
        "build vocabulary, and numericalize tokens into tensor.\n",
        "\n",
        "To run this tutorial, first install spacy using pip or conda. Next,\n",
        "download the raw data for the English and German Spacy tokenizers from\n",
        "https://spacy.io/usage/models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhB8GQdG6Qcb"
      },
      "source": [
        "import math\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torch import Tensor\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIGjFwaS4XK6"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 128\n",
        "PAD_IDX = 3#de_vocab['<pad>']\n",
        "BOS_IDX = 2#de_vocab['<bos>']\n",
        "EOS_IDX = 1#de_vocab['<eos>']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IfKlmodNtLo",
        "outputId": "27421477-7a91-4467-e1aa-dc329e38182d"
      },
      "source": [
        "logger.info('')\n",
        "logger.info('='*50)\n",
        "logger.info('DATA PROCESSING')\n",
        "logger.info('')\n",
        "\n",
        "train_data_new = []\n",
        "for iter in range(len(datas['train_source.txt'])):\n",
        "    train_data_new.append((torch.Tensor(datas['train_source.txt']['source'][iter]).type(torch.long),torch.Tensor(datas['train_target.txt']['target'][iter]).type(torch.long)))\n",
        "\n",
        "logger.info('Train data processing finished')\n",
        "\n",
        "test_data_new = []\n",
        "for iter in range(len(datas['test_source.txt'])):\n",
        "    test_data_new.append((torch.Tensor(datas['test_source.txt']['source'][iter]).type(torch.long),torch.Tensor(datas['test_target.txt']['target'][iter]).type(torch.long)))\n",
        "\n",
        "logger.info('Test data processing finished')\n",
        "logger.info('-'*50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:38,801 - \n",
            "2021-06-10 13:30:38,803 - ==================================================\n",
            "2021-06-10 13:30:38,805 - DATA PROCESSING\n",
            "2021-06-10 13:30:38,812 - \n",
            "2021-06-10 13:30:39,164 - Train data processing finished\n",
            "2021-06-10 13:30:39,245 - Test data processing finished\n",
            "2021-06-10 13:30:39,247 - --------------------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KVYiRDz4XK7"
      },
      "source": [
        "DataLoader\n",
        "----------\n",
        "\n",
        "The last torch specific feature we’ll use is the DataLoader, which is\n",
        "easy to use since it takes the data as its first argument. Specifically,\n",
        "as the docs say: DataLoader combines a dataset and a sampler, and\n",
        "provides an iterable over the given dataset. The DataLoader supports\n",
        "both map-style and iterable-style datasets with single- or multi-process\n",
        "loading, customizing loading order and optional automatic batching\n",
        "(collation) and memory pinning.\n",
        "\n",
        "Please pay attention to collate_fn (optional) that merges a list of\n",
        "samples to form a mini-batch of Tensor(s). Used when using batched\n",
        "loading from a map-style dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOHn0G2l4XK7"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    \n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)#, batch_first=True)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)#, batch_first=True)\n",
        "\n",
        "  return de_batch, en_batch\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbIYZSrhN3bZ"
      },
      "source": [
        "\n",
        "train_iter_new = DataLoader(train_data_new, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter_new = DataLoader(test_data_new, batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, collate_fn=generate_batch)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYUyCpMmg558",
        "outputId": "7723e4da-909f-4577-a9f8-b9fcbb34d2ad"
      },
      "source": [
        "logger.info('DATA LOAD FINISHED')\n",
        "logger.info('='*50)\n",
        "logger.info('')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:39,280 - DATA LOAD FINISHED\n",
            "2021-06-10 13:30:39,282 - ==================================================\n",
            "2021-06-10 13:30:39,284 - \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv7MsLOq4XK8"
      },
      "source": [
        "Transformer!\n",
        "------------\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
        "need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
        "paper for solving machine translation task. Transformer model consists\n",
        "of an encoder and decoder block each containing fixed number of layers.\n",
        "\n",
        "Encoder processes the input sequence by propogating it, through a series\n",
        "of Multi-head Attention and Feed forward network layers. The output from\n",
        "the Encoder referred to as ``memory``, is fed to the decoder along with\n",
        "target tensors. Encoder and decoder are trained in an end-to-end fashion\n",
        "using teacher forcing technique.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlmMFl14XK8"
      },
      "source": [
        "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
        "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "                \n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My1dSjBV4XK9"
      },
      "source": [
        "Text tokens are represented by using token embeddings. Positional\n",
        "encoding is added to the token embedding to introduce a notion of word\n",
        "order.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOVcW82k4XK9"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + \n",
        "                            self.pos_embedding[:token_embedding.size(0),:])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev9ghczM4XK9"
      },
      "source": [
        "We create a ``subsequent word`` mask to stop a target word from\n",
        "attending to its subsequent words. We also create masks, for masking\n",
        "source and target padding tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44lUEXyc4XK-"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[0]\n",
        "  tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF_SCurr4XK-"
      },
      "source": [
        "Define model parameters and instantiate model \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx30efur4XK-"
      },
      "source": [
        "# SRC_VOCAB_SIZE = len(de_vocab)\n",
        "# TGT_VOCAB_SIZE = len(en_vocab)\n",
        "\n",
        "SRC_VOCAB_SIZE = 1000#len(de_vocab)\n",
        "TGT_VOCAB_SIZE = 1000#len(en_vocab)\n",
        "\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "NUM_EPOCHS = 100#64#16\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-qU872u3Bkr",
        "outputId": "2168b939-1a89-464d-9c0b-cb511d5f7aeb"
      },
      "source": [
        "# 모델의 state_dict 출력\n",
        "logger.info(\"Model's state_dict:\")\n",
        "for param_tensor in transformer.state_dict():\n",
        "    logger.info(f\"{param_tensor}\\t {transformer.state_dict()[param_tensor].size()}\")\n",
        "\n",
        "# 옵티마이저의 state_dict 출력\n",
        "logger.info(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    logger.info(f\"{var_name} \\t {optimizer.state_dict()[var_name]}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:42,144 - Model's state_dict:\n",
            "2021-06-10 13:30:42,149 - transformer_encoder.layers.0.self_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,152 - transformer_encoder.layers.0.self_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,155 - transformer_encoder.layers.0.self_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,158 - transformer_encoder.layers.0.self_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,161 - transformer_encoder.layers.0.linear1.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,164 - transformer_encoder.layers.0.linear1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,167 - transformer_encoder.layers.0.linear2.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,170 - transformer_encoder.layers.0.linear2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,173 - transformer_encoder.layers.0.norm1.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,176 - transformer_encoder.layers.0.norm1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,179 - transformer_encoder.layers.0.norm2.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,183 - transformer_encoder.layers.0.norm2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,185 - transformer_encoder.layers.1.self_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,189 - transformer_encoder.layers.1.self_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,191 - transformer_encoder.layers.1.self_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,194 - transformer_encoder.layers.1.self_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,198 - transformer_encoder.layers.1.linear1.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,201 - transformer_encoder.layers.1.linear1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,204 - transformer_encoder.layers.1.linear2.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,208 - transformer_encoder.layers.1.linear2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,211 - transformer_encoder.layers.1.norm1.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,214 - transformer_encoder.layers.1.norm1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,217 - transformer_encoder.layers.1.norm2.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,220 - transformer_encoder.layers.1.norm2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,222 - transformer_encoder.layers.2.self_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,226 - transformer_encoder.layers.2.self_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,229 - transformer_encoder.layers.2.self_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,232 - transformer_encoder.layers.2.self_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,236 - transformer_encoder.layers.2.linear1.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,239 - transformer_encoder.layers.2.linear1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,241 - transformer_encoder.layers.2.linear2.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,245 - transformer_encoder.layers.2.linear2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,248 - transformer_encoder.layers.2.norm1.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,251 - transformer_encoder.layers.2.norm1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,254 - transformer_encoder.layers.2.norm2.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,257 - transformer_encoder.layers.2.norm2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,260 - transformer_decoder.layers.0.self_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,265 - transformer_decoder.layers.0.self_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,268 - transformer_decoder.layers.0.self_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,271 - transformer_decoder.layers.0.self_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,275 - transformer_decoder.layers.0.multihead_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,279 - transformer_decoder.layers.0.multihead_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,282 - transformer_decoder.layers.0.multihead_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,285 - transformer_decoder.layers.0.multihead_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,288 - transformer_decoder.layers.0.linear1.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,292 - transformer_decoder.layers.0.linear1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,296 - transformer_decoder.layers.0.linear2.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,299 - transformer_decoder.layers.0.linear2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,302 - transformer_decoder.layers.0.norm1.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,305 - transformer_decoder.layers.0.norm1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,309 - transformer_decoder.layers.0.norm2.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,313 - transformer_decoder.layers.0.norm2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,315 - transformer_decoder.layers.0.norm3.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,319 - transformer_decoder.layers.0.norm3.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,323 - transformer_decoder.layers.1.self_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,326 - transformer_decoder.layers.1.self_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,332 - transformer_decoder.layers.1.self_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,337 - transformer_decoder.layers.1.self_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,341 - transformer_decoder.layers.1.multihead_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,344 - transformer_decoder.layers.1.multihead_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,347 - transformer_decoder.layers.1.multihead_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,351 - transformer_decoder.layers.1.multihead_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,354 - transformer_decoder.layers.1.linear1.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,357 - transformer_decoder.layers.1.linear1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,361 - transformer_decoder.layers.1.linear2.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,364 - transformer_decoder.layers.1.linear2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,368 - transformer_decoder.layers.1.norm1.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,371 - transformer_decoder.layers.1.norm1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,374 - transformer_decoder.layers.1.norm2.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,378 - transformer_decoder.layers.1.norm2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,382 - transformer_decoder.layers.1.norm3.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,386 - transformer_decoder.layers.1.norm3.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,389 - transformer_decoder.layers.2.self_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,392 - transformer_decoder.layers.2.self_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,395 - transformer_decoder.layers.2.self_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,399 - transformer_decoder.layers.2.self_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,402 - transformer_decoder.layers.2.multihead_attn.in_proj_weight\t torch.Size([1536, 512])\n",
            "2021-06-10 13:30:42,406 - transformer_decoder.layers.2.multihead_attn.in_proj_bias\t torch.Size([1536])\n",
            "2021-06-10 13:30:42,409 - transformer_decoder.layers.2.multihead_attn.out_proj.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,412 - transformer_decoder.layers.2.multihead_attn.out_proj.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,416 - transformer_decoder.layers.2.linear1.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,419 - transformer_decoder.layers.2.linear1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,423 - transformer_decoder.layers.2.linear2.weight\t torch.Size([512, 512])\n",
            "2021-06-10 13:30:42,426 - transformer_decoder.layers.2.linear2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,429 - transformer_decoder.layers.2.norm1.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,432 - transformer_decoder.layers.2.norm1.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,435 - transformer_decoder.layers.2.norm2.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,438 - transformer_decoder.layers.2.norm2.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,441 - transformer_decoder.layers.2.norm3.weight\t torch.Size([512])\n",
            "2021-06-10 13:30:42,445 - transformer_decoder.layers.2.norm3.bias\t torch.Size([512])\n",
            "2021-06-10 13:30:42,447 - generator.weight\t torch.Size([1000, 512])\n",
            "2021-06-10 13:30:42,451 - generator.bias\t torch.Size([1000])\n",
            "2021-06-10 13:30:42,455 - src_tok_emb.embedding.weight\t torch.Size([1000, 512])\n",
            "2021-06-10 13:30:42,459 - tgt_tok_emb.embedding.weight\t torch.Size([1000, 512])\n",
            "2021-06-10 13:30:42,460 - positional_encoding.pos_embedding\t torch.Size([5000, 1, 512])\n",
            "2021-06-10 13:30:42,463 - Optimizer's state_dict:\n",
            "2021-06-10 13:30:42,465 - state \t {}\n",
            "2021-06-10 13:30:42,467 - param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.98), 'eps': 1e-09, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]}]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhE3Uzbf4XK-"
      },
      "source": [
        "def train_epoch(model, train_iter, optimizer):\n",
        "  model.train()\n",
        "  losses = 0\n",
        "  for idx, (src, tgt) in enumerate(train_iter):\n",
        "      src = src.to(device)\n",
        "      tgt = tgt.to(device)\n",
        "      #print(src.shape)\n",
        "      tgt_input = tgt[:-1, :]\n",
        "\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      tgt_out = tgt[1:,:]\n",
        "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      losses += loss.item()\n",
        "  return losses / len(train_iter)\n",
        "\n",
        "\n",
        "def evaluate(model, val_iter):\n",
        "  model.eval()\n",
        "  losses = 0\n",
        "  for idx, (src, tgt) in (enumerate(val_iter)):\n",
        "    src = src.to(device)\n",
        "    tgt = tgt.to(device)\n",
        "\n",
        "    tgt_input = tgt[:-1, :]\n",
        "\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    tgt_out = tgt[1:,:]\n",
        "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    losses += loss.item()\n",
        "  return losses / len(val_iter)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VicevZIST_5Z"
      },
      "source": [
        "def evaluate_new(model, val_iter):\n",
        "  model.eval()\n",
        "  losses = 0\n",
        "  for idx, (src, tgt) in (enumerate(val_iter)):\n",
        "    src = src.to(device)\n",
        "    tgt = tgt.to(device)\n",
        "\n",
        "    tgt_input = tgt[:-1, :]\n",
        "\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    tgt_out = tgt[1:,:]\n",
        "    #loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "    #losses += loss.item()\n",
        "\n",
        "    #print(logits.reshape(-1, logits.shape[-1]).shape)\n",
        "    pred = torch.argmax(logits.reshape(-1, logits.shape[-1]), dim=1).view(tgt_out.shape)\n",
        "\n",
        "    y = (torch.transpose(tgt_out, 0, 1))\n",
        "    new_pred = torch.transpose(pred,0,1)\n",
        "\n",
        "    #print(y)\n",
        "    new_losses = 0\n",
        "    for i in range(len(new_pred)):\n",
        "        try:\n",
        "            fin = new_pred[i].tolist().index(1)\n",
        "        except:\n",
        "            fin = len(new_pred[i])\n",
        "            if fin==0:\n",
        "                fin=1\n",
        "        new_loss=0\n",
        "        new_loss = np.linalg.norm(new_pred[i][:fin].cpu()-y[i][:fin].cpu())\n",
        "        #fin = len(new_pred[i])\n",
        "        # for k in range(fin):\n",
        "        #     output = new_pred[i][k]\n",
        "        #     target = y[i][k]\n",
        "        #     new_loss += ((output - target)**2)#.mean()\n",
        "        #     #print(new_loss)\n",
        "        new_losses += new_loss#((new_loss/fin).item())\n",
        "        \n",
        "    losses += (new_losses / len(new_pred))    \n",
        "    \n",
        "  return losses / len(val_iter)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5i0tYKc4XK_"
      },
      "source": [
        "##Train model \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwZKwDAD4XLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1fadd2-9574-4f77-d0b2-282bed913866"
      },
      "source": [
        "logger.info('='*50)\n",
        "logger.info('TRAIN')\n",
        "logger.info('-'*50)\n",
        "train_losses = []\n",
        "logger.info(f'NUM EPOCHS: {NUM_EPOCHS}')\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "  start_time = time.time()\n",
        "  train_loss = train_epoch(transformer, train_iter_new, optimizer)\n",
        "  end_time = time.time()\n",
        "  train_losses.append(train_loss)\n",
        "  #val_loss = evaluate(transformer, test_iter_new)\n",
        "  #norm_loss = evaluate_new(transformer,test_iter_new)\n",
        "  #print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}\"))#, Val loss: {val_loss:.3f}, \"\n",
        "          #f\"Epoch time = {(end_time - start_time):.3f}s\"),'norm_loss :',norm_loss)\n",
        "  logger.info((f'Epoch: {epoch}, Train loss: {train_loss:.3f} Epoch time = {(end_time - start_time):.3f}s'))\n",
        "logger.info('-'*50)\n",
        "logger.info('TRAIN FINISHED')\n",
        "logger.info('='*50)\n",
        "logger.info('')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:30:42,557 - ==================================================\n",
            "2021-06-10 13:30:42,559 - TRAIN\n",
            "2021-06-10 13:30:42,561 - --------------------------------------------------\n",
            "2021-06-10 13:30:42,566 - NUM EPOCHS: 100\n",
            "2021-06-10 13:30:50,110 - Epoch: 1, Train loss: 4.253 Epoch time = 7.543s\n",
            "2021-06-10 13:30:57,616 - Epoch: 2, Train loss: 3.167 Epoch time = 7.504s\n",
            "2021-06-10 13:31:05,138 - Epoch: 3, Train loss: 2.719 Epoch time = 7.520s\n",
            "2021-06-10 13:31:12,606 - Epoch: 4, Train loss: 2.386 Epoch time = 7.466s\n",
            "2021-06-10 13:31:20,184 - Epoch: 5, Train loss: 2.099 Epoch time = 7.576s\n",
            "2021-06-10 13:31:27,717 - Epoch: 6, Train loss: 1.841 Epoch time = 7.531s\n",
            "2021-06-10 13:31:35,237 - Epoch: 7, Train loss: 1.615 Epoch time = 7.517s\n",
            "2021-06-10 13:31:42,714 - Epoch: 8, Train loss: 1.420 Epoch time = 7.474s\n",
            "2021-06-10 13:31:50,191 - Epoch: 9, Train loss: 1.262 Epoch time = 7.475s\n",
            "2021-06-10 13:31:57,607 - Epoch: 10, Train loss: 1.125 Epoch time = 7.414s\n",
            "2021-06-10 13:32:05,179 - Epoch: 11, Train loss: 1.018 Epoch time = 7.570s\n",
            "2021-06-10 13:32:12,716 - Epoch: 12, Train loss: 0.922 Epoch time = 7.535s\n",
            "2021-06-10 13:32:20,134 - Epoch: 13, Train loss: 0.848 Epoch time = 7.415s\n",
            "2021-06-10 13:32:27,664 - Epoch: 14, Train loss: 0.778 Epoch time = 7.529s\n",
            "2021-06-10 13:32:35,226 - Epoch: 15, Train loss: 0.717 Epoch time = 7.560s\n",
            "2021-06-10 13:32:42,742 - Epoch: 16, Train loss: 0.666 Epoch time = 7.514s\n",
            "2021-06-10 13:32:50,219 - Epoch: 17, Train loss: 0.619 Epoch time = 7.476s\n",
            "2021-06-10 13:32:57,706 - Epoch: 18, Train loss: 0.569 Epoch time = 7.485s\n",
            "2021-06-10 13:33:05,194 - Epoch: 19, Train loss: 0.533 Epoch time = 7.486s\n",
            "2021-06-10 13:33:12,717 - Epoch: 20, Train loss: 0.498 Epoch time = 7.520s\n",
            "2021-06-10 13:33:20,195 - Epoch: 21, Train loss: 0.467 Epoch time = 7.476s\n",
            "2021-06-10 13:33:27,674 - Epoch: 22, Train loss: 0.431 Epoch time = 7.476s\n",
            "2021-06-10 13:33:35,216 - Epoch: 23, Train loss: 0.410 Epoch time = 7.540s\n",
            "2021-06-10 13:33:42,692 - Epoch: 24, Train loss: 0.378 Epoch time = 7.473s\n",
            "2021-06-10 13:33:50,148 - Epoch: 25, Train loss: 0.355 Epoch time = 7.454s\n",
            "2021-06-10 13:33:57,627 - Epoch: 26, Train loss: 0.338 Epoch time = 7.478s\n",
            "2021-06-10 13:34:05,119 - Epoch: 27, Train loss: 0.307 Epoch time = 7.489s\n",
            "2021-06-10 13:34:12,630 - Epoch: 28, Train loss: 0.292 Epoch time = 7.509s\n",
            "2021-06-10 13:34:20,128 - Epoch: 29, Train loss: 0.277 Epoch time = 7.496s\n",
            "2021-06-10 13:34:27,725 - Epoch: 30, Train loss: 0.260 Epoch time = 7.595s\n",
            "2021-06-10 13:34:35,240 - Epoch: 31, Train loss: 0.245 Epoch time = 7.513s\n",
            "2021-06-10 13:34:42,734 - Epoch: 32, Train loss: 0.225 Epoch time = 7.491s\n",
            "2021-06-10 13:34:50,235 - Epoch: 33, Train loss: 0.219 Epoch time = 7.500s\n",
            "2021-06-10 13:34:57,657 - Epoch: 34, Train loss: 0.203 Epoch time = 7.419s\n",
            "2021-06-10 13:35:05,160 - Epoch: 35, Train loss: 0.190 Epoch time = 7.501s\n",
            "2021-06-10 13:35:12,589 - Epoch: 36, Train loss: 0.185 Epoch time = 7.427s\n",
            "2021-06-10 13:35:20,081 - Epoch: 37, Train loss: 0.168 Epoch time = 7.490s\n",
            "2021-06-10 13:35:27,525 - Epoch: 38, Train loss: 0.161 Epoch time = 7.442s\n",
            "2021-06-10 13:35:35,069 - Epoch: 39, Train loss: 0.152 Epoch time = 7.542s\n",
            "2021-06-10 13:35:42,617 - Epoch: 40, Train loss: 0.145 Epoch time = 7.546s\n",
            "2021-06-10 13:35:50,086 - Epoch: 41, Train loss: 0.136 Epoch time = 7.467s\n",
            "2021-06-10 13:35:57,550 - Epoch: 42, Train loss: 0.131 Epoch time = 7.462s\n",
            "2021-06-10 13:36:05,009 - Epoch: 43, Train loss: 0.127 Epoch time = 7.457s\n",
            "2021-06-10 13:36:12,540 - Epoch: 44, Train loss: 0.114 Epoch time = 7.529s\n",
            "2021-06-10 13:36:20,074 - Epoch: 45, Train loss: 0.114 Epoch time = 7.531s\n",
            "2021-06-10 13:36:27,607 - Epoch: 46, Train loss: 0.106 Epoch time = 7.531s\n",
            "2021-06-10 13:36:35,120 - Epoch: 47, Train loss: 0.100 Epoch time = 7.510s\n",
            "2021-06-10 13:36:42,667 - Epoch: 48, Train loss: 0.093 Epoch time = 7.545s\n",
            "2021-06-10 13:36:50,239 - Epoch: 49, Train loss: 0.093 Epoch time = 7.570s\n",
            "2021-06-10 13:36:57,701 - Epoch: 50, Train loss: 0.086 Epoch time = 7.460s\n",
            "2021-06-10 13:37:05,220 - Epoch: 51, Train loss: 0.084 Epoch time = 7.516s\n",
            "2021-06-10 13:37:12,758 - Epoch: 52, Train loss: 0.086 Epoch time = 7.536s\n",
            "2021-06-10 13:37:20,229 - Epoch: 53, Train loss: 0.073 Epoch time = 7.469s\n",
            "2021-06-10 13:37:27,733 - Epoch: 54, Train loss: 0.072 Epoch time = 7.502s\n",
            "2021-06-10 13:37:35,286 - Epoch: 55, Train loss: 0.068 Epoch time = 7.552s\n",
            "2021-06-10 13:37:42,777 - Epoch: 56, Train loss: 0.067 Epoch time = 7.489s\n",
            "2021-06-10 13:37:50,273 - Epoch: 57, Train loss: 0.067 Epoch time = 7.493s\n",
            "2021-06-10 13:37:57,775 - Epoch: 58, Train loss: 0.062 Epoch time = 7.500s\n",
            "2021-06-10 13:38:05,299 - Epoch: 59, Train loss: 0.057 Epoch time = 7.521s\n",
            "2021-06-10 13:38:12,877 - Epoch: 60, Train loss: 0.058 Epoch time = 7.576s\n",
            "2021-06-10 13:38:20,356 - Epoch: 61, Train loss: 0.056 Epoch time = 7.476s\n",
            "2021-06-10 13:38:27,892 - Epoch: 62, Train loss: 0.054 Epoch time = 7.534s\n",
            "2021-06-10 13:38:35,449 - Epoch: 63, Train loss: 0.053 Epoch time = 7.555s\n",
            "2021-06-10 13:38:43,006 - Epoch: 64, Train loss: 0.053 Epoch time = 7.554s\n",
            "2021-06-10 13:38:50,570 - Epoch: 65, Train loss: 0.050 Epoch time = 7.562s\n",
            "2021-06-10 13:38:58,146 - Epoch: 66, Train loss: 0.047 Epoch time = 7.574s\n",
            "2021-06-10 13:39:05,539 - Epoch: 67, Train loss: 0.047 Epoch time = 7.391s\n",
            "2021-06-10 13:39:13,013 - Epoch: 68, Train loss: 0.047 Epoch time = 7.473s\n",
            "2021-06-10 13:39:20,536 - Epoch: 69, Train loss: 0.044 Epoch time = 7.521s\n",
            "2021-06-10 13:39:28,068 - Epoch: 70, Train loss: 0.043 Epoch time = 7.530s\n",
            "2021-06-10 13:39:35,716 - Epoch: 71, Train loss: 0.040 Epoch time = 7.646s\n",
            "2021-06-10 13:39:43,199 - Epoch: 72, Train loss: 0.041 Epoch time = 7.481s\n",
            "2021-06-10 13:39:50,687 - Epoch: 73, Train loss: 0.041 Epoch time = 7.486s\n",
            "2021-06-10 13:39:58,172 - Epoch: 74, Train loss: 0.037 Epoch time = 7.483s\n",
            "2021-06-10 13:40:05,621 - Epoch: 75, Train loss: 0.037 Epoch time = 7.446s\n",
            "2021-06-10 13:40:13,154 - Epoch: 76, Train loss: 0.037 Epoch time = 7.531s\n",
            "2021-06-10 13:40:20,670 - Epoch: 77, Train loss: 0.040 Epoch time = 7.514s\n",
            "2021-06-10 13:40:28,157 - Epoch: 78, Train loss: 0.036 Epoch time = 7.485s\n",
            "2021-06-10 13:40:35,604 - Epoch: 79, Train loss: 0.033 Epoch time = 7.445s\n",
            "2021-06-10 13:40:43,166 - Epoch: 80, Train loss: 0.033 Epoch time = 7.559s\n",
            "2021-06-10 13:40:50,790 - Epoch: 81, Train loss: 0.032 Epoch time = 7.622s\n",
            "2021-06-10 13:40:58,343 - Epoch: 82, Train loss: 0.033 Epoch time = 7.551s\n",
            "2021-06-10 13:41:05,841 - Epoch: 83, Train loss: 0.031 Epoch time = 7.496s\n",
            "2021-06-10 13:41:13,405 - Epoch: 84, Train loss: 0.031 Epoch time = 7.562s\n",
            "2021-06-10 13:41:20,927 - Epoch: 85, Train loss: 0.031 Epoch time = 7.520s\n",
            "2021-06-10 13:41:28,400 - Epoch: 86, Train loss: 0.029 Epoch time = 7.470s\n",
            "2021-06-10 13:41:35,831 - Epoch: 87, Train loss: 0.029 Epoch time = 7.429s\n",
            "2021-06-10 13:41:43,358 - Epoch: 88, Train loss: 0.027 Epoch time = 7.525s\n",
            "2021-06-10 13:41:50,893 - Epoch: 89, Train loss: 0.028 Epoch time = 7.533s\n",
            "2021-06-10 13:41:58,457 - Epoch: 90, Train loss: 0.028 Epoch time = 7.563s\n",
            "2021-06-10 13:42:05,979 - Epoch: 91, Train loss: 0.028 Epoch time = 7.519s\n",
            "2021-06-10 13:42:13,448 - Epoch: 92, Train loss: 0.028 Epoch time = 7.468s\n",
            "2021-06-10 13:42:21,010 - Epoch: 93, Train loss: 0.026 Epoch time = 7.559s\n",
            "2021-06-10 13:42:28,542 - Epoch: 94, Train loss: 0.026 Epoch time = 7.531s\n",
            "2021-06-10 13:42:36,110 - Epoch: 95, Train loss: 0.024 Epoch time = 7.566s\n",
            "2021-06-10 13:42:43,665 - Epoch: 96, Train loss: 0.025 Epoch time = 7.552s\n",
            "2021-06-10 13:42:51,174 - Epoch: 97, Train loss: 0.023 Epoch time = 7.507s\n",
            "2021-06-10 13:42:58,685 - Epoch: 98, Train loss: 0.024 Epoch time = 7.508s\n",
            "2021-06-10 13:43:06,219 - Epoch: 99, Train loss: 0.024 Epoch time = 7.532s\n",
            "2021-06-10 13:43:13,792 - Epoch: 100, Train loss: 0.022 Epoch time = 7.572s\n",
            "2021-06-10 13:43:13,795 - --------------------------------------------------\n",
            "2021-06-10 13:43:13,798 - TRAIN FINISHED\n",
            "2021-06-10 13:43:13,800 - ==================================================\n",
            "2021-06-10 13:43:13,802 - \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na2Jk-SFQzLx",
        "outputId": "36361378-3eb2-473c-c439-7aeda15a6a83"
      },
      "source": [
        "torch.save(transformer, f'trained_model/transformer_epoch_{NUM_EPOCHS}.pth')\n",
        "logger.info(f'MODEL SAVED AS transformer_epoch_{NUM_EPOCHS}.pth')\n",
        "logger.info('')\n",
        "test_loss = evaluate(transformer, test_iter_new)\n",
        "logger.info(f'TEST LOSS : {test_loss}')\n",
        "logger.info('')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:43:14,065 - MODEL SAVED AS transformer_epoch_100.pth\n",
            "2021-06-10 13:43:14,070 - \n",
            "2021-06-10 13:43:14,813 - TEST LOSS : 0.3388785906136036\n",
            "2021-06-10 13:43:14,815 - \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaKtC-iv30H_"
      },
      "source": [
        "#Decode and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpWepFl4XLB"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "          break\n",
        "    return ys\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5SWArPKc2LV"
      },
      "source": [
        "def translate(model, idx):\n",
        "  model.eval()\n",
        "  tokens = test_data_new[idx][0]\n",
        "  num_tokens = len(tokens)\n",
        "  src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "  src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "  tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "  return tokens.tolist(),test_data_new[idx][1].tolist(),tgt_tokens[1:-1].tolist()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra8kI6A-xUuf",
        "outputId": "12efc3ba-aeee-4b8d-c4b9-d3834c1a7d92"
      },
      "source": [
        "import random\n",
        "\n",
        "logger.info(f\"PREDICTION {10} EXAMPLES \")\n",
        "logger.info('='*50)\n",
        "for i in range(10):\n",
        "    idx = random.randrange(1,len(test_data_new))\n",
        "    source, target, predict = translate(transformer,idx)\n",
        "    logger.info(f\"> {source}\")\n",
        "    logger.info(f\"= {target}\")\n",
        "    logger.info(f\"< {predict}\")\n",
        "    logger.info('-'*40)\n",
        "logger.info('')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:43:14,862 - PREDICTION 10 EXAMPLES \n",
            "2021-06-10 13:43:14,864 - ==================================================\n",
            "2021-06-10 13:43:14,963 - > [271, 584, 68, 311, 584, 342, 68, 227, 200, 156, 437, 68, 263, 105, 95, 140, 227, 271, 200, 311, 68, 52, 200, 95, 271, 35, 113]\n",
            "2021-06-10 13:43:14,964 - = [370, 68, 5, 68, 81, 68, 25, 85, 70, 158, 68, 331, 108]\n",
            "2021-06-10 13:43:14,966 - < [370, 68, 5, 68, 81, 68, 247, 85, 70, 158, 68, 331, 149, 551]\n",
            "2021-06-10 13:43:14,969 - ----------------------------------------\n",
            "2021-06-10 13:43:15,057 - > [33, 95, 437, 156, 105, 584, 342, 35, 68, 437, 157, 402, 200, 402, 437, 78, 437, 157, 140]\n",
            "2021-06-10 13:43:15,059 - = [240, 211, 405, 189, 85, 68, 430, 16, 158, 199, 438, 359]\n",
            "2021-06-10 13:43:15,062 - < [240, 211, 266, 158, 108, 68, 430, 16, 158, 199, 438, 359]\n",
            "2021-06-10 13:43:15,063 - ----------------------------------------\n",
            "2021-06-10 13:43:15,131 - > [140, 105, 52, 619, 437, 140]\n",
            "2021-06-10 13:43:15,132 - = [222, 476]\n",
            "2021-06-10 13:43:15,134 - < [222, 476, 476, 476, 476, 476, 476, 476, 476]\n",
            "2021-06-10 13:43:15,137 - ----------------------------------------\n",
            "2021-06-10 13:43:15,278 - > [311, 437, 35, 327, 68, 311, 584, 342, 68, 52, 200, 157, 68, 416, 584, 584, 619, 68, 200, 140, 68, 140, 227, 437, 35, 437, 68, 304, 584, 95, 68, 105, 157, 304, 584, 95, 78, 200, 140, 105, 584, 157, 327]\n",
            "2021-06-10 13:43:15,281 - = [276, 85, 68, 5, 68, 527, 68, 494, 68, 97, 68, 19, 108, 68, 241, 68, 430, 241, 406, 158, 118]\n",
            "2021-06-10 13:43:15,284 - < [276, 85, 68, 5, 68, 527, 68, 494, 68, 97, 68, 569, 68, 290, 85, 68, 430, 241, 406, 158, 118]\n",
            "2021-06-10 13:43:15,286 - ----------------------------------------\n",
            "2021-06-10 13:43:15,388 - > [35, 227, 584, 95, 140, 437, 35, 140, 68, 29, 200, 311]\n",
            "2021-06-10 13:43:15,391 - = [122, 222, 85, 359, 68, 503, 158]\n",
            "2021-06-10 13:43:15,393 - < [122, 222, 85, 359, 68, 503, 158, 68, 170, 503, 158, 108, 68, 189, 241]\n",
            "2021-06-10 13:43:15,396 - ----------------------------------------\n",
            "2021-06-10 13:43:15,464 - > [105, 157, 140, 437, 95, 157, 200, 140, 105, 584, 157, 200, 416, 68, 78, 200, 105, 416]\n",
            "2021-06-10 13:43:15,466 - = [430, 299, 180, 282, 568, 68, 406, 155]\n",
            "2021-06-10 13:43:15,467 - < [430, 299, 180, 282, 568, 68, 406, 155, 194, 213]\n",
            "2021-06-10 13:43:15,470 - ----------------------------------------\n",
            "2021-06-10 13:43:15,546 - > [227, 437, 453, 35, 68, 200, 68, 271, 200, 95, 619, 68, 227, 584, 95, 35, 437, 327]\n",
            "2021-06-10 13:43:15,551 - = [153, 108, 68, 189, 68, 89, 152, 68, 154, 85]\n",
            "2021-06-10 13:43:15,553 - < [153, 108, 68, 189, 68, 149, 443, 68, 154, 85]\n",
            "2021-06-10 13:43:15,556 - ----------------------------------------\n",
            "2021-06-10 13:43:15,665 - > [584, 33, 437, 157, 68, 311, 584, 342, 95, 68, 78, 584, 342, 140, 227, 68, 29, 105, 271, 437, 327]\n",
            "2021-06-10 13:43:15,667 - = [321, 27, 68, 5, 189, 68, 43, 86, 85, 68, 137, 158, 149]\n",
            "2021-06-10 13:43:15,670 - < [321, 27, 68, 5, 189, 68, 245, 449, 68, 137, 158, 149, 68, 137, 430]\n",
            "2021-06-10 13:43:15,671 - ----------------------------------------\n",
            "2021-06-10 13:43:15,750 - > [78, 311, 68, 52, 200, 95, 68, 105, 35, 68, 78, 105, 35, 35, 105, 157, 402, 327]\n",
            "2021-06-10 13:43:15,752 - = [43, 158, 68, 331, 68, 158, 108, 68, 517, 17]\n",
            "2021-06-10 13:43:15,755 - < [43, 158, 68, 331, 68, 158, 108, 68, 517, 82, 178]\n",
            "2021-06-10 13:43:15,759 - ----------------------------------------\n",
            "2021-06-10 13:43:15,853 - > [402, 584, 68, 140, 584, 402, 437, 140, 227, 437, 95]\n",
            "2021-06-10 13:43:15,855 - = [54, 86, 68, 359, 16, 569]\n",
            "2021-06-10 13:43:15,856 - < [54, 86, 68, 359, 16, 569, 68, 415, 211, 68, 359, 16, 189, 54]\n",
            "2021-06-10 13:43:15,858 - ----------------------------------------\n",
            "2021-06-10 13:43:15,861 - \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjsJ1HKOpZpQ"
      },
      "source": [
        "def correct_num(model): #whole answer\n",
        "  model.eval()\n",
        "  corrects = 0\n",
        "  for idx in range(len(test_data_new)):\n",
        "    tokens = test_data_new[idx][0]\n",
        "    num_tokens = len(tokens)\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "    if test_data_new[idx][1].tolist()==tgt_tokens[1:-1].tolist():\n",
        "        corrects +=1\n",
        "  return corrects"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeTRAajjryr0"
      },
      "source": [
        "def correct_num_wide(model): #sliced answer \n",
        "  model.eval()\n",
        "  corrects = 0\n",
        "  for idx in range(len(test_data_new)):\n",
        "    tokens = test_data_new[idx][0]\n",
        "    num_tokens = len(tokens)\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "    if test_data_new[idx][1].tolist()==tgt_tokens[1:len(test_data_new[idx][1])+1].tolist():\n",
        "        corrects +=1\n",
        "  return corrects"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw_XUaAqQmNh",
        "outputId": "38c5240c-edc2-4d12-ec2f-2038af380351"
      },
      "source": [
        "acc_c = (correct_num(transformer)) #epoch 16\n",
        "acc_cw = (correct_num_wide(transformer))\n",
        "logger.info('='*50)\n",
        "logger.info(f'ACCURACY of model transformer_epoch_{NUM_EPOCHS}.pth')\n",
        "logger.info('-'*50)\n",
        "logger.info(f'Truely corrected target sequence score : {acc_c/len(test_data_new)} {acc_c} / {len(test_data_new)}')\n",
        "\n",
        "logger.info(f'Widely corrected target sequence score : {acc_cw/len(test_data_new)} {acc_cw} / {len(test_data_new)}')\n",
        "logger.info('')\n",
        "logger.info('FINISHED')\n",
        "logger.info('')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 13:49:01,408 - ==================================================\n",
            "2021-06-10 13:49:01,411 - ACCURACY of model transformer_epoch_100.pth\n",
            "2021-06-10 13:49:01,415 - --------------------------------------------------\n",
            "2021-06-10 13:49:01,417 - Truely corrected target sequence score : 0.2455 491 / 2000\n",
            "2021-06-10 13:49:01,421 - Widely corrected target sequence score : 0.6435 1287 / 2000\n",
            "2021-06-10 13:49:01,423 - \n",
            "2021-06-10 13:49:01,426 - FINISHED\n",
            "2021-06-10 13:49:01,427 - \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSWp9D7d4XLC"
      },
      "source": [
        "References\n",
        "----------\n",
        "\n",
        "1. Attention is all you need paper.\n",
        "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding \n",
        "\n"
      ]
    }
  ]
}